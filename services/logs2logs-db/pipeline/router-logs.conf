input {
  udp {
    port => 5140
    type => syslog
    queue_size => 5000
    receive_buffer_bytes => 26214400
  }
  lumberjack {
    port => 5044
    ssl_certificate => "certs/lumberjack.cert"
    ssl_key => "certs/lumberjack.key"
    codec => json
  }
}

filter {
  grok {
    match => ["message", "(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{SYSLOGPROG}: %{HAPROXYHTTPBASE}"]
  }
  # If the above grok has an error and it contains "be_sni/" or "be_no_sni/" inside the message, this request is an internal haproxy forward for HTTPs requests
  # we can savely drop them as they don't contain any information and will be logged twice anyway
  if "_grokparsefailure" in [tags] and [message] =~ /(be_sni\/)|(be_no_sni\/)/ {
    drop { }
  }
  grok {
    match => ["captured_request_headers", "%{URIHOST:request_header_host}\|%{GREEDYDATA:request_header_useragent}"]
  }
  grok {
    match => {"backend_name" => [ "%{NOTSPACE:haproxy_backend}:%{NOTSPACE:openshift_project}:%{NOTSPACE:openshift_route}", "%{NOTSPACE:openshift_project}" ] }
  }
  grok {
    match => {"server_name" => [ "pod:%{NOTSPACE:openshift_pod}:%{NOTSPACE:openshift_service}:%{NOTSPACE:openshift_pod_ip}:%{NOTSPACE:openshift_pod_port}", "%{NOTSPACE:server_name}" ] }
  }
  if ![openshift_project] {
    mutate {
      add_field => { "openshift_project" => "errors" }
    }
  }
  # We run automated testing which creates project names in the form of e2e-0000000000-AAAAAAAAAAAAAAAAAAAAAAAAAA
  # we just save them as with the project name 'e2'
  if [openshift_project] =~ /^e2e-\d{10}-\S{26}$/ {
    mutate {
      update => { "openshift_project" => "e2e" }
    }
  }
}

output {
  # stdout { codec => rubydebug }
  elasticsearch {
    user => admin
    password => "${LOGSDB_ADMIN_PASSWORD}"
    hosts => ["${ELASTICSEARCH_URL}"]
    index => "router-logs-%{[openshift_project]}-%{+xxxx.ww}" # index per week
    template => "/usr/share/logstash/templates/router-logs.json"
    template_name => "router-logs"
    template_overwrite => true
  }
}
